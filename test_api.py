# import requests
# from llama_cpp import Llama
#
# # ========== é…ç½® ==========
# server_url = "http://192.168.199.195:8000/generate"
# original_prompt = "please help me plan a solo two day trip in Tokyo"
#
# local_model_path = "D:/Downloads/Phi-3.5-mini-instruct-Q6_K_L.gguf"
#
# llm_local = Llama(
#     model_path=local_model_path,
#     n_gpu_layers=32,
#     n_ctx=2048,
#     n_batch=512,
#     main_gpu=0,
#     offload_kqv=True,
#     verbose=True
# )
#
# # ========== 1ï¸âƒ£ å‘é€è¯·æ±‚åˆ°æœåŠ¡å™¨ ==========
# payload = {"prompt": original_prompt}
# response = requests.post(server_url, json=payload)
#
# if response.status_code == 200:
#     result = response.json()
#     sketch = result["sketch"]
#     print("âœ… Received sketch from server:\n")
#     print(sketch)
# else:
#     print(f"âŒ Request failed with status code {response.status_code}")
#     print(response.text)
#     exit(1)
#
# # ========== 2ï¸âƒ£ æ„é€ æœ¬åœ° SLM æŒ‡ä»¤ ==========
# instruction_prompt = (
#     "You are a helpful assistant. Your task is to carefully read the user's original request "
#     "and a sketch draft generated by the server. The sketch may be incomplete. "
#     "Based on the sketch and the user's original intent, please provide a detailed, refined, and helpful response.\n\n"
#     "Additionally, due to output token limitations, your final response must fit within approximately 520 tokens to ensure that the user receives a concise yet complete answer.\n\n"
#     f"ğŸ”¹ User's original request:\n{original_prompt}\n\n"
#     f"ğŸ”¹ Server's sketch draft:\n{sketch}\n\n"
#     "âœ… Please now write a complete, detailed, and concise plan tailored to the user's original request, while keeping your answer within approximately 520 tokens."
# )
#
# print("\nğŸ”” Composed instruction prompt for local SLM:\n")
# print(instruction_prompt)
#
# # ========== 3ï¸âƒ£ æœ¬åœ° SLM æ¨ç† ==========
# resp_local = llm_local(
#     instruction_prompt,
#     temperature=0.7,
#     top_p=0.9,
#     stop=["<|endoftext|>"],
#     max_tokens=512
# )
#
# final_output = resp_local["choices"][0]["text"]
#
# # ========== 4ï¸âƒ£ è¾“å‡ºæœ€ç»ˆç»“æœ ==========
# print("\nâœ… Final completed plan from local SLM:\n")
# print(final_output)



from edge_utils import EdgeProcessor
from llama_cpp import Llama
import requests

# ===== 1ï¸âƒ£ åˆå§‹åŒ– EdgeProcessor =====
processor = EdgeProcessor(
    model_name="microsoft/phi-3.5-mini",
    risk_threshold=0.5,
    delta_threshold=0.1,
    device="cuda"  # æˆ– "cpu" å¦‚æœæ²¡æœ‰ GPU
)

# ===== 2ï¸âƒ£ åŸå§‹ prompt =====
original_prompt = "please help me plan a solo two day trip in Tokyo"

# ===== 3ï¸âƒ£ NER + Risk analysis + Selective encryption =====
entities = processor.run_ner(original_prompt)
risk = processor.compute_risk_score(entities)
print(f"ğŸ” Detected entities: {entities}, Risk score: {risk:.2f}")

if risk >= processor.risk_threshold:
    protected_prompt = processor.encrypt_sensitive_entities(original_prompt, entities)
    print(f"ğŸ”’ Protected prompt after encryption:\n{protected_prompt}")
else:
    protected_prompt = original_prompt
    print("âœ… No sensitive entities found or below risk threshold, using original prompt.")

# ===== 4ï¸âƒ£ å‘é€ protected prompt åˆ°æœåŠ¡å™¨ API =====
server_url = "http://192.168.199.195:8000/generate"
payload = {"prompt": protected_prompt}
response = requests.post(server_url, json=payload)

if response.status_code == 200:
    sketch = response.json()["sketch"]
    print(f"\nâœ… Received sketch from server:\n{sketch}")
else:
    print(f"âŒ Server request failed: {response.status_code}")
    exit(1)

# ===== 5ï¸âƒ£ æœ¬åœ° SLM ç»§ç»­è¡¥å…¨ =====
# åˆå§‹åŒ–æœ¬åœ° SLM
local_model_path = r"D:\Program Files\LLM\phi-3.5-mini-instruct-Q6_K_L.gguf"
llm_local = Llama(
    model_path=local_model_path,
    n_gpu_layers=32,
    n_ctx=2048,
    n_batch=512,
    main_gpu=0,
    offload_kqv=True,
    verbose=True
)

# ===== 6ï¸âƒ£ æ„é€  instruction prompt =====
instruction_prompt = (
    "You are a helpful assistant. Your task is to carefully read the user's original request "
    "and a sketch draft generated by the server. The sketch may be incomplete and have some erors since it might recieve a noisy prompt comparing to the orginal one . "
    "Based on the sketch and the user's original intent, please provide a detailed, refined, and helpful response.\n\n"
    "Additionally, due to output token limitations, your final response must fit within approximately 520 tokens to ensure that the user receives a concise yet complete answer.\n\n"
    f"ğŸ”¹ User's original request:\n{original_prompt}\n\n"
    f"ğŸ”¹ Server's sketch draft:\n{sketch}\n\n"
    "âœ… Please now write a complete, detailed, and concise plan tailored to the user's original request, while keeping your answer within approximately 520 tokens."
)

print("\nğŸ”” Composed instruction prompt for local SLM:\n")
print(instruction_prompt)

# ===== 7ï¸âƒ£ SLM æ¨ç† =====
resp_local = llm_local(
    instruction_prompt,
    temperature=0.7,
    top_p=0.9,
    stop=["<|endoftext|>"],
    max_tokens=512
)

final_output = resp_local["choices"][0]["text"]

# ===== 8ï¸âƒ£ è¾“å‡ºæœ€ç»ˆç»“æœ =====
print("\nâœ… Final completed plan from local SLM:\n")
print(final_output)
