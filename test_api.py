# import requests
# from llama_cpp import Llama
#
# # ========== 配置 ==========
# server_url = "http://192.168.199.195:8000/generate"
# original_prompt = "please help me plan a solo two day trip in Tokyo"
#
# local_model_path = "D:/Downloads/Phi-3.5-mini-instruct-Q6_K_L.gguf"
#
# llm_local = Llama(
#     model_path=local_model_path,
#     n_gpu_layers=32,
#     n_ctx=2048,
#     n_batch=512,
#     main_gpu=0,
#     offload_kqv=True,
#     verbose=True
# )
#
# # ========== 1️⃣ 发送请求到服务器 ==========
# payload = {"prompt": original_prompt}
# response = requests.post(server_url, json=payload)
#
# if response.status_code == 200:
#     result = response.json()
#     sketch = result["sketch"]
#     print("✅ Received sketch from server:\n")
#     print(sketch)
# else:
#     print(f"❌ Request failed with status code {response.status_code}")
#     print(response.text)
#     exit(1)
#
# # ========== 2️⃣ 构造本地 SLM 指令 ==========
# instruction_prompt = (
#     "You are a helpful assistant. Your task is to carefully read the user's original request "
#     "and a sketch draft generated by the server. The sketch may be incomplete. "
#     "Based on the sketch and the user's original intent, please provide a detailed, refined, and helpful response.\n\n"
#     "Additionally, due to output token limitations, your final response must fit within approximately 520 tokens to ensure that the user receives a concise yet complete answer.\n\n"
#     f"🔹 User's original request:\n{original_prompt}\n\n"
#     f"🔹 Server's sketch draft:\n{sketch}\n\n"
#     "✅ Please now write a complete, detailed, and concise plan tailored to the user's original request, while keeping your answer within approximately 520 tokens."
# )
#
# print("\n🔔 Composed instruction prompt for local SLM:\n")
# print(instruction_prompt)
#
# # ========== 3️⃣ 本地 SLM 推理 ==========
# resp_local = llm_local(
#     instruction_prompt,
#     temperature=0.7,
#     top_p=0.9,
#     stop=["<|endoftext|>"],
#     max_tokens=512
# )
#
# final_output = resp_local["choices"][0]["text"]
#
# # ========== 4️⃣ 输出最终结果 ==========
# print("\n✅ Final completed plan from local SLM:\n")
# print(final_output)



from edge_utils import EdgeProcessor
from llama_cpp import Llama
import requests

# ===== 1️⃣ 初始化 EdgeProcessor =====
processor = EdgeProcessor(
    model_name="microsoft/phi-3.5-mini",
    risk_threshold=0.5,
    delta_threshold=0.1,
    device="cuda"  # 或 "cpu" 如果没有 GPU
)

# ===== 2️⃣ 原始 prompt =====
original_prompt = "please help me plan a solo two day trip in Tokyo"

# ===== 3️⃣ NER + Risk analysis + Selective encryption =====
entities = processor.run_ner(original_prompt)
risk = processor.compute_risk_score(entities)
print(f"🔎 Detected entities: {entities}, Risk score: {risk:.2f}")

if risk >= processor.risk_threshold:
    protected_prompt = processor.encrypt_sensitive_entities(original_prompt, entities)
    print(f"🔒 Protected prompt after encryption:\n{protected_prompt}")
else:
    protected_prompt = original_prompt
    print("✅ No sensitive entities found or below risk threshold, using original prompt.")

# ===== 4️⃣ 发送 protected prompt 到服务器 API =====
server_url = "http://192.168.199.195:8000/generate"
payload = {"prompt": protected_prompt}
response = requests.post(server_url, json=payload)

if response.status_code == 200:
    sketch = response.json()["sketch"]
    print(f"\n✅ Received sketch from server:\n{sketch}")
else:
    print(f"❌ Server request failed: {response.status_code}")
    exit(1)

# ===== 5️⃣ 本地 SLM 继续补全 =====
# 初始化本地 SLM
local_model_path = r"D:\Program Files\LLM\phi-3.5-mini-instruct-Q6_K_L.gguf"
llm_local = Llama(
    model_path=local_model_path,
    n_gpu_layers=32,
    n_ctx=2048,
    n_batch=512,
    main_gpu=0,
    offload_kqv=True,
    verbose=True
)

# ===== 6️⃣ 构造 instruction prompt =====
instruction_prompt = (
    "You are a helpful assistant. Your task is to carefully read the user's original request "
    "and a sketch draft generated by the server. The sketch may be incomplete and have some erors since it might recieve a noisy prompt comparing to the orginal one . "
    "Based on the sketch and the user's original intent, please provide a detailed, refined, and helpful response.\n\n"
    "Additionally, due to output token limitations, your final response must fit within approximately 520 tokens to ensure that the user receives a concise yet complete answer.\n\n"
    f"🔹 User's original request:\n{original_prompt}\n\n"
    f"🔹 Server's sketch draft:\n{sketch}\n\n"
    "✅ Please now write a complete, detailed, and concise plan tailored to the user's original request, while keeping your answer within approximately 520 tokens."
)

print("\n🔔 Composed instruction prompt for local SLM:\n")
print(instruction_prompt)

# ===== 7️⃣ SLM 推理 =====
resp_local = llm_local(
    instruction_prompt,
    temperature=0.7,
    top_p=0.9,
    stop=["<|endoftext|>"],
    max_tokens=512
)

final_output = resp_local["choices"][0]["text"]

# ===== 8️⃣ 输出最终结果 =====
print("\n✅ Final completed plan from local SLM:\n")
print(final_output)
